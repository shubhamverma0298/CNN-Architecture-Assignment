{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CNN Architecture Assignment**"
      ],
      "metadata": {
        "id": "b8juFbKUdfmb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q 1 What is the role of filters and feature maps in Convolutional Neural\n",
        "Network (CNN) ?**\n",
        "  - In a CNN, filters act as feature detectors, sliding across the input to find specific patterns like edges or textures, while feature maps are the output of these filters, highlighting the presence and location of detected features within the input data.\n",
        "  - Role of Filters\n",
        "    1. Feature Detection\n",
        "    2. Specialization\n",
        "    3. Learning Capabilities\n",
        "    4. Creating Feature Maps\n",
        "  - Role of Feature Maps\n",
        "    1. Feature Representation\n",
        "    2. Highlighting Feature Locations\n",
        "    3. Building Complex Representations\n",
        "    4. Enabling Decision-Making\n",
        "\n",
        "**Q 2 Explain the concepts of padding and stride in CNNs(Convolutional Neural\n",
        "Network). How do they affect the output dimensions of feature maps ?**\n",
        "  - In Convolutional Neural Networks (CNNs), padding and stride are hyperparameters that control the behavior of the convolution operation and significantly influence the output dimensions of feature maps.\n",
        "  - Padding: Padding involves adding extra pixels (typically zeros) around the border of the input image before applying the convolution filter.\n",
        "  - Stride: Stride defines the number of pixels by which the convolution filter shifts across the input image at each step.\n",
        "  - Effect:\n",
        "  1. A stride of 1 means the filter moves one pixel at a time, resulting in a larger output feature map.\n",
        "  2. A larger stride (e.g., 2 or 3) means the filter skips pixels, leading to a smaller output feature map and reducing the computational cost\n",
        "\n",
        "**Q 3 Define receptive field in the context of CNNs. Why is it important for deep architectures?**\n",
        "  - The receptive field, in contrast, is an emergent property that describes the cumulative region of the original input that affects a single neuron's output after multiple convolutional and pooling layers.\n",
        "  - In a Convolutional Neural Network (CNN), the receptive field is the specific area of the input image or data that a single neuron or feature in a given layer can \"see\" and respond to.\n",
        "  - **Importance** provides a way of detecting contrast, and is used for detecting objects' edges.\n",
        "\n",
        "**Q 4 Discuss how filter size and stride influence the number of parameters in a\n",
        "CNN ?**\n",
        "  - Filter size and stride are crucial hyperparameters in Convolutional Neural Networks (CNNs) that influence the network's behavior and performance, though their impact on the number of parameters differs significantly.\n",
        "  - Filter Size and Number of Parameters:\n",
        "     Direct Impact: The filter (or kernel) size directly impacts the number of parameters in a convolutional layer. Each filter has a set of trainable weights, and the number of these weights is determined by its dimensions.\n",
        "  - Stride and Number of Parameters:\n",
        "  - No Direct Impact: Stride, which defines how many pixels the filter shifts across the input at each step, does not directly influence the number of parameters within a convolutional layer. The parameters of a filter (its weights and bias) are fixed regardless of how many times it is applied or how large the step size is.\n",
        "\n",
        "**Q 5 Compare and contrast different CNN-based architectures like LeNet,\n",
        "AlexNet, and VGG in terms of depth, filter sizes, and performance?**\n",
        "  - LeNet, AlexNet, and VGG represent a progression in Convolutional Neural Network (CNN) architectures, each building upon the previous in terms of depth, filter strategies, and ultimately, performance on image classification tasks.\n",
        "  - Depth:\n",
        "    1. LeNet-5: A relatively shallow network, typically consisting of 7 layers (including convolutional, pooling, and fully connected layers). It was designed for specific tasks like digit recognition.\n",
        "    2. AlexNet: Significantly deeper than LeNet, featuring 8 layers (5 convolutional, 3 fully connected). This increased depth allowed it to learn more complex features.\n",
        "    3. VGG: Known for its extreme depth, with common variants like VGG-16 and VGG-19, indicating 16 and 19 weight layers respectively. This marked a trend towards very deep architectures.\n",
        "  - Filter Sizes:\n",
        "    1. LeNet-5: Utilized larger filter sizes in its early layers (e.g., 5x5) to capture broad features, followed by smaller filters.\n",
        "    2. AlexNet:Employed a mix of filter sizes, notably a large 11x11 filter in the first convolutional layer, followed by smaller 5x5 and 3x3 filters.\n",
        "    3. VGG: A key characteristic of VGG is its exclusive use of very small 3x3 convolutional filters throughout the network. This approach, while increasing the number of layers, allowed for multiple non-linear transformations and smaller receptive fields, effectively simulating larger filters with fewer parameters.\n",
        "  - Performance:\n",
        "    1. LeNet-5: Achieved remarkable performance for its time on tasks like handwritten digit recognition (e.g., MNIST dataset), demonstrating the power of CNNs.\n",
        "    2. AlexNet: Revolutionized image classification by winning the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) in 2012 with a significantly lower error rate than previous methods, showcasing the potential of deep CNNs on large-scale datasets.\n",
        "    3. VGG: Further improved upon AlexNet's performance in ILSVRC, consistently achieving high accuracy rates and setting new benchmarks for image classification, particularly with its consistent and deep architecture."
      ],
      "metadata": {
        "id": "VdqF1HWgfeGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q 6 Using keras, build and train a simple CNN model on the MNIST dataset\n",
        "from scratch. Include code for module creation, compilation, training, and evaluation ?**"
      ],
      "metadata": {
        "id": "93QJtyMKjAAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "f8hIGmzldn9J"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83acdfd3"
      },
      "source": [
        "### Load and Preprocess the MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "644fbf27",
        "outputId": "cd3c6d4c-d89d-4d8d-acb1-0d2817aa337d"
      },
      "source": [
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Preprocess the data\n",
        "# Add a channel dimension to the images (for grayscale, it's 1)\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\n",
        "# Convert pixel values to float32 and normalize to [0, 1]\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoded format\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape}\")\n",
        "print(f\"y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape}\")\n",
        "print(f\"y_test shape: {y_test.shape}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "x_train shape: (60000, 28, 28, 1)\n",
            "y_train shape: (60000, 10)\n",
            "x_test shape: (10000, 28, 28, 1)\n",
            "y_test shape: (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "347112ec"
      },
      "source": [
        "### Build the CNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "a78a0f71",
        "outputId": "4682a2ed-3cd8-4f48-aa01-602f9eded140"
      },
      "source": [
        "# Define the CNN model architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax') # 10 classes for digits 0-9\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m204,928\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m225,034\u001b[0m (879.04 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">225,034</span> (879.04 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fcbe8de"
      },
      "source": [
        "### Compile the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "defe1c2e"
      },
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55a0fe98"
      },
      "source": [
        "### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcaa05b0",
        "outputId": "8350ff95-a803-4d89-8562-90beffab1bc7"
      },
      "source": [
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=10, batch_size=32, validation_split=0.2)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.8980 - loss: 0.3328 - val_accuracy: 0.9835 - val_loss: 0.0603\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.9849 - loss: 0.0490 - val_accuracy: 0.9873 - val_loss: 0.0451\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9900 - loss: 0.0308 - val_accuracy: 0.9880 - val_loss: 0.0443\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9921 - loss: 0.0234 - val_accuracy: 0.9889 - val_loss: 0.0403\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9949 - loss: 0.0152 - val_accuracy: 0.9883 - val_loss: 0.0506\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9955 - loss: 0.0153 - val_accuracy: 0.9887 - val_loss: 0.0498\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.9969 - loss: 0.0092 - val_accuracy: 0.9888 - val_loss: 0.0470\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.9979 - loss: 0.0063 - val_accuracy: 0.9886 - val_loss: 0.0501\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.9978 - loss: 0.0059 - val_accuracy: 0.9876 - val_loss: 0.0524\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9985 - loss: 0.0048 - val_accuracy: 0.9895 - val_loss: 0.0526\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0edb0f7"
      },
      "source": [
        "### Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ae30dc0c",
        "outputId": "9ab2db63-b167-4532-e6da-2bafc65b3eb0"
      },
      "source": [
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print(f\"Test Loss: {loss:.4f}\")\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.0326\n",
            "Test Accuracy: 0.9920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q 7 Load and preprocess the CIFAR-10 dataset using Keras, and create a\n",
        "CNN model to classify RGB images. Show your preprocessing and architecture?**"
      ],
      "metadata": {
        "id": "msCWZQ8QkBh8"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76cf47b9"
      },
      "source": [
        "## Load and preprocess data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "363adf5e",
        "outputId": "319bd3a3-c807-442e-ff26-a47a35203024"
      },
      "source": [
        "from tensorflow.keras.datasets import cifar10\n",
        "\n",
        "# Load the CIFAR-10 dataset\n",
        "(x_train_cifar, y_train_cifar), (x_test_cifar, y_test_cifar) = cifar10.load_data()\n",
        "\n",
        "# Normalize the image data\n",
        "x_train_cifar = x_train_cifar.astype('float32') / 255.0\n",
        "x_test_cifar = x_test_cifar.astype('float32') / 255.0\n",
        "\n",
        "# Convert labels to one-hot encoded format\n",
        "y_train_cifar = tf.keras.utils.to_categorical(y_train_cifar, num_classes=10)\n",
        "y_test_cifar = tf.keras.utils.to_categorical(y_test_cifar, num_classes=10)\n",
        "\n",
        "# Print the shapes of the preprocessed data\n",
        "print(f\"x_train_cifar shape: {x_train_cifar.shape}\")\n",
        "print(f\"y_train_cifar shape: {y_train_cifar.shape}\")\n",
        "print(f\"x_test_cifar shape: {x_test_cifar.shape}\")\n",
        "print(f\"y_test_cifar shape: {y_test_cifar.shape}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n",
            "x_train_cifar shape: (50000, 32, 32, 3)\n",
            "y_train_cifar shape: (50000, 10)\n",
            "x_test_cifar shape: (10000, 32, 32, 3)\n",
            "y_test_cifar shape: (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9b1ec7bd"
      },
      "source": [
        "## Build the cnn model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "1c8d0678",
        "outputId": "baf483a7-291e-4566-d112-1e83a4a97c4f"
      },
      "source": [
        "# Define the CNN model architecture for CIFAR-10\n",
        "model_cifar = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10, activation='softmax') # 10 classes for CIFAR-10\n",
        "])\n",
        "\n",
        "# Print the model summary\n",
        "model_cifar.summary()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/convolutional/base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m896\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m15\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m64\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │       \u001b[38;5;34m295,040\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             │         \u001b[38;5;34m1,290\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">295,040</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m315,722\u001b[0m (1.20 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">315,722</span> (1.20 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m315,722\u001b[0m (1.20 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">315,722</span> (1.20 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "061b3ac0"
      },
      "source": [
        "## Compile the model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "125a988c"
      },
      "source": [
        "# Compile the CIFAR-10 model\n",
        "model_cifar.compile(optimizer='adam',\n",
        "                    loss='categorical_crossentropy',\n",
        "                    metrics=['accuracy'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01f72006"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "### Subtask:\n",
        "Train the compiled model on the preprocessed CIFAR-10 training data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce45b50b"
      },
      "source": [
        "**Reasoning**:\n",
        "Train the compiled CNN model on the preprocessed CIFAR-10 training data with specified epochs, batch size, and validation split, and store the training history.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60e7383d",
        "outputId": "ca120f03-0203-4c88-c8db-ab058eff6fce"
      },
      "source": [
        "# Train the model\n",
        "history_cifar = model_cifar.fit(x_train_cifar, y_train_cifar,\n",
        "                                epochs=10,\n",
        "                                batch_size=64,\n",
        "                                validation_split=0.2)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 8ms/step - accuracy: 0.3545 - loss: 1.7675 - val_accuracy: 0.5518 - val_loss: 1.2840\n",
            "Epoch 2/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.5612 - loss: 1.2394 - val_accuracy: 0.6072 - val_loss: 1.1185\n",
            "Epoch 3/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6311 - loss: 1.0641 - val_accuracy: 0.6441 - val_loss: 1.0286\n",
            "Epoch 4/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.6694 - loss: 0.9556 - val_accuracy: 0.6609 - val_loss: 0.9933\n",
            "Epoch 5/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.6948 - loss: 0.8767 - val_accuracy: 0.6554 - val_loss: 0.9997\n",
            "Epoch 6/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7225 - loss: 0.8007 - val_accuracy: 0.6802 - val_loss: 0.9329\n",
            "Epoch 7/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7416 - loss: 0.7484 - val_accuracy: 0.6807 - val_loss: 0.9337\n",
            "Epoch 8/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7637 - loss: 0.6832 - val_accuracy: 0.6877 - val_loss: 0.9368\n",
            "Epoch 9/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7803 - loss: 0.6322 - val_accuracy: 0.6905 - val_loss: 0.9335\n",
            "Epoch 10/10\n",
            "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8014 - loss: 0.5688 - val_accuracy: 0.6832 - val_loss: 0.9703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac2e9b08"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b96fa088",
        "outputId": "7e069d2a-3f6a-40bd-fe92-95c4a163dec8"
      },
      "source": [
        "# Evaluate the model on the test set\n",
        "loss_cifar, accuracy_cifar = model_cifar.evaluate(x_test_cifar, y_test_cifar, verbose=0)\n",
        "\n",
        "print(f\"Test Loss (CIFAR-10): {loss_cifar:.4f}\")\n",
        "print(f\"Test Accuracy (CIFAR-10): {accuracy_cifar:.4f}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss (CIFAR-10): 0.9833\n",
            "Test Accuracy (CIFAR-10): 0.6796\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q 8 Using PyTorch, write a script to define and train a CNN on the MNIST\n",
        "dataset. Include model definition, data loaders, training loop, and accuracy evaluation?**"
      ],
      "metadata": {
        "id": "UCD3Dyyklahn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 1. Define the CNN Model\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128) # 7*7 comes from the image size after pooling\n",
        "        self.fc2 = nn.Linear(128, 10) # 10 classes for MNIST\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = x.view(-1, 64 * 7 * 7) # Flatten the output\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# 2. Data Loaders\n",
        "# Define transformations for the MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(), # Convert images to tensors\n",
        "    transforms.Normalize((0.1307,), (0.3081,)) # Normalize the data\n",
        "])\n",
        "\n",
        "# Load the MNIST training and test datasets\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
        "\n",
        "# 3. Training Loop\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model_pytorch = SimpleCNN()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_pytorch.parameters(), lr=0.001)\n",
        "\n",
        "# Training function\n",
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
        "\n",
        "# 4. Accuracy Evaluation\n",
        "# Evaluation function\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print(f'\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} ({100. * correct / len(test_loader.dataset):.0f}%)\\n')\n",
        "\n",
        "# Set up device (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_pytorch.to(device)\n",
        "\n",
        "# Train the model\n",
        "epochs = 5 # You can adjust the number of epochs\n",
        "for epoch in range(1, epochs + 1):\n",
        "    train(model_pytorch, device, train_loader, optimizer, epoch)\n",
        "\n",
        "# Evaluate the model\n",
        "test(model_pytorch, device, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5N3KlxY3lfBb",
        "outputId": "3daef0f9-d826-4bd7-93e0-cdf2f9e3ccb1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 11.6MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 337kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.22MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.74MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.317183\n",
            "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.426227\n",
            "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.051520\n",
            "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.017507\n",
            "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.071224\n",
            "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.064645\n",
            "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.018438\n",
            "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.007600\n",
            "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.035749\n",
            "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.109772\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.075633\n",
            "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.021895\n",
            "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.013789\n",
            "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.000956\n",
            "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.014279\n",
            "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.013750\n",
            "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.008664\n",
            "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.021705\n",
            "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.054172\n",
            "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.018312\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.010620\n",
            "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.002257\n",
            "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.019603\n",
            "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.009183\n",
            "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.011634\n",
            "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.006473\n",
            "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.005300\n",
            "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.002521\n",
            "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.033813\n",
            "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.021289\n",
            "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.006301\n",
            "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.049171\n",
            "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.002586\n",
            "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.056229\n",
            "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.055089\n",
            "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.004705\n",
            "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.026955\n",
            "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.004458\n",
            "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.006209\n",
            "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.021494\n",
            "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.007468\n",
            "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.015713\n",
            "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.000288\n",
            "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.001639\n",
            "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.006225\n",
            "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.001637\n",
            "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.003263\n",
            "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.000697\n",
            "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.053877\n",
            "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.000769\n",
            "\n",
            "Test set: Average loss: 0.0000, Accuracy: 9878/10000 (99%)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q 9 Given a custom image dataset stored in a local directory, write code using\n",
        "Keras ImageDataGenerator to preprocess and train a CNN model?**"
      ],
      "metadata": {
        "id": "NRcjz1_Zl6VG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import os"
      ],
      "metadata": {
        "id": "zRP8aIusm63Y"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "\n",
        "# Google Drive file ID\n",
        "file_id = \"1TCU1nqgIe1R_dW6LTkRxlufHlCCazyJl\"\n",
        "# Download destination filename\n",
        "output = \"myfile.zip\"\n",
        "\n",
        "# Download the file\n",
        "gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "-sFjCL-inj4l",
        "outputId": "6a8b33ff-0550-4495-d305-a2a4083a5d10"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1TCU1nqgIe1R_dW6LTkRxlufHlCCazyJl\n",
            "From (redirected): https://drive.google.com/uc?id=1TCU1nqgIe1R_dW6LTkRxlufHlCCazyJl&confirm=t&uuid=899063f6-d7db-411f-b6dd-7ed1052eabda\n",
            "To: /content/myfile.zip\n",
            "100%|██████████| 63.9M/63.9M [00:01<00:00, 56.5MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'myfile.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"myfile.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"dataset\")\n"
      ],
      "metadata": {
        "id": "_w2f3FgonrLH"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define path (update with your datasets)\n",
        "train_dir=\"/content/dataset/train\"\n",
        "val_dir= \"/content/dataset/valid\"\n",
        "img_size= 299\n",
        "batch_size= 32"
      ],
      "metadata": {
        "id": "2DP8bqLOnw6S"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data generators\n",
        "train_datagen = ImageDataGenerator(\n",
        "    preprocessing_function= tf.keras.applications.xception.preprocess_input,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    horizontal_flip=True\n",
        ")\n",
        "val_datagen= ImageDataGenerator(\n",
        "    preprocessing_function= tf.keras.applications.xception.preprocess_input\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_dir,\n",
        "    target_size=(img_size,img_size),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_datagen.flow_from_directory(\n",
        "    val_dir,\n",
        "    target_size=(img_size,img_size),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "es3wMUV4nyaW",
        "outputId": "dbedb67d-e5b0-4e0c-977a-5b70812eac74"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1275 images belonging to 2 classes.\n",
            "Found 364 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load base model\n",
        "base_model = Xception(weights='imagenet',include_top=False , input_shape=(img_size,img_size,3))\n",
        "base_model.trainable= False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1pCvbyrInz09",
        "outputId": "1e99d76a-6306-4f79-b0f1-32cfe725476b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m83683744/83683744\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add custom classifiers\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(train_generator.num_classes,activation='softmax')(x)\n",
        "model= Model(inputs= base_model.input,outputs=predictions)"
      ],
      "metadata": {
        "id": "Ji4GtsWrn3RC"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "AaiupUgKn7bk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=10,\n",
        "    validation_data=val_generator\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ly5VisdOn8sA",
        "outputId": "d69a10bb-3493-47d3-969c-96a26b51f397"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 841ms/step - accuracy: 0.9265 - loss: 0.1922 - val_accuracy: 0.9313 - val_loss: 0.1406\n",
            "Epoch 2/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 864ms/step - accuracy: 0.9461 - loss: 0.1577 - val_accuracy: 0.9313 - val_loss: 0.1327\n",
            "Epoch 3/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 833ms/step - accuracy: 0.9309 - loss: 0.1552 - val_accuracy: 0.9341 - val_loss: 0.1280\n",
            "Epoch 4/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 836ms/step - accuracy: 0.9457 - loss: 0.1406 - val_accuracy: 0.9368 - val_loss: 0.1262\n",
            "Epoch 5/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 861ms/step - accuracy: 0.9396 - loss: 0.1362 - val_accuracy: 0.9313 - val_loss: 0.1246\n",
            "Epoch 6/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 852ms/step - accuracy: 0.9560 - loss: 0.1230 - val_accuracy: 0.9423 - val_loss: 0.1227\n",
            "Epoch 7/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 840ms/step - accuracy: 0.9422 - loss: 0.1282 - val_accuracy: 0.9341 - val_loss: 0.1239\n",
            "Epoch 8/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 837ms/step - accuracy: 0.9592 - loss: 0.1111 - val_accuracy: 0.9231 - val_loss: 0.1330\n",
            "Epoch 9/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 873ms/step - accuracy: 0.9439 - loss: 0.1306 - val_accuracy: 0.9396 - val_loss: 0.1208\n",
            "Epoch 10/10\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 888ms/step - accuracy: 0.9624 - loss: 0.1184 - val_accuracy: 0.9423 - val_loss: 0.1205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q 10 You are working on a web application for a medical imaging startup. Your\n",
        "task is to build and deploy a CNN model that classifies chest X-ray images into “Normal”\n",
        "and “Pneumonia” categories. Describe your end-to-end approach–from data preparation\n",
        "and model training to deploying the model as a web app using Streamlit ?**\n",
        "\n",
        "- Data Preparation and Preprocessing\n",
        "This is the foundational step. The quality of our data directly impacts the model's performance.\n",
        "\n",
        "1. Dataset Acquisition: We'll use a publicly available dataset of chest X-ray images, such as the one from Kaggle, which is already split into \"Normal\" and \"Pneumonia\" categories. This dataset contains a mix of images, so we'll need to handle the structure and potential class imbalance.\n",
        "\n",
        "2. Data Augmentation: Medical datasets are often small, which can lead to overfitting. We'll combat this by artificially expanding our training data. This involves applying various random transformations to the images, like rotation, zooming, flipping, and shearing. This not only increases the number of images but also helps the model generalize better to unseen data.\n",
        "\n",
        "3. Image Resizing and Normalization: All images must have a consistent size to be fed into the CNN. We'll resize them to a standard dimension (e.g., 224x224 pixels). Additionally, we'll normalize the pixel values by scaling them to a range like [0, 1]. This helps the model converge faster and improves training stability.\n",
        "\n",
        "4. Creating Data Generators: We'll use a library like Keras's ImageDataGenerator to efficiently load images from their directories, apply the augmentations on the fly, and create batches of data for training. This is memory-efficient as it avoids loading the entire dataset into RAM at once.\n",
        "\n",
        "- Model Development and Training\n",
        "Instead of building a CNN from scratch, we'll use a powerful and efficient pre-trained model.\n",
        "\n",
        "1. Transfer Learning: Transfer learning involves using a model trained on a large, general-purpose dataset (like ImageNet) as a starting point. We'll use a model architecture like DenseNet121 or ResNet50. These models have already learned to recognize fundamental features like edges, textures, and shapes, which are also useful for analyzing X-ray images.\n",
        "\n",
        "2. Model Architecture:\n",
        "\n",
        "We'll load the pre-trained model (e.g., DenseNet121) without its top classification layer. This \"frozen\" part of the network acts as a powerful feature extractor.\n",
        "\n",
        "We'll then add our own custom classification layers on top. This typically includes a global average pooling layer to reduce dimensions, a dense (fully connected) layer for classification, and a final dense layer with a sigmoid activation function to output a probability for the \"Pneumonia\" class.\n",
        "\n",
        "We'll compile the model using an optimizer like Adam and a binary cross-entropy loss function, which is suitable for binary classification.\n",
        "\n",
        "Training and Validation: We'll train the model on our prepared dataset. During training, we'll monitor its performance on a separate validation set to check for overfitting. We'll use callbacks like ModelCheckpoint to save the best-performing model and EarlyStopping to halt training if the validation loss stops decreasing, preventing wasted time and resources.\n",
        "\n",
        "- Model Deployment with Streamlit\n",
        "Once the model is trained and saved, we'll create a user-friendly web application to demonstrate its functionality.\n",
        "\n",
        "Setting up the Streamlit App:\n",
        "\n",
        "We'll create a main Python file (app.py) for the web application.\n",
        "\n",
        "Using Streamlit, we'll create a simple user interface with a title, a description, and an image uploader widget.\n",
        "\n",
        "1. Integrating the Model:\n",
        "\n",
        "The trained model will be loaded into the Streamlit app. To avoid reloading the model every time the app re-renders, we'll use Streamlit's @st.cache_resource decorator. This caches the model, making the application much faster.\n",
        "\n",
        "When a user uploads an image, the app will read the file and preprocess it to match the input requirements of our trained CNN model (e.g., resize to 224x224 and normalize).\n",
        "\n",
        "The preprocessed image will be passed to the model for prediction. The model will output a probability value.\n",
        "\n",
        "2. Displaying the Results:\n",
        "\n",
        "The app will display the uploaded image.\n",
        "\n",
        "Based on the model's prediction, the app will show a clear and concise result, such as \"Prediction: Pneumonia\" or \"Prediction: Normal.\" We can also display the confidence score (the predicted probability) to provide more context.\n",
        "\n",
        "3. Deployment:\n",
        "\n",
        "We'll put all the necessary files (the Streamlit app, the trained model file, a requirements.txt listing all dependencies) into a single folder and push it to a GitHub repository.\n",
        "\n",
        "Using Streamlit's Community Cloud, we'll connect the repository, and the app will be automatically built and deployed. This provides a live URL for the web application, allowing anyone to access and test our model."
      ],
      "metadata": {
        "id": "1Vl6NYzRoahG"
      }
    }
  ]
}